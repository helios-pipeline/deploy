

--- ./main.tf ---

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
  required_version = ">= 1.2.0"
}

provider "aws" {
  region  = "us-west-1"
  profile = "capstone-team4"
}

module "vpc" {
  source = "./modules/vpc"
}

module "clickhouse_ec2_instance" {
  source = "./modules/clickhouse_ec2_instance"
  vpc_id    = module.vpc.vpc_id
  subnet_id = module.vpc.private_subnet_ids[0]
}

module "flask_ec2_instance" {
  source           = "./modules/flask_ec2_instance"
  vpc_id           = module.vpc.vpc_id
  subnet_id        = module.vpc.public_subnet_ids[0]
  webapp_public_ip = module.clickhouse_ec2_instance.webapp_public_ip
  depends_on       = [module.clickhouse_ec2_instance]
}

module "dynamodb_instance" {
  source = "./modules/dynamodb_instance"
}

module "lambda_function" {
  source           = "./modules/lambda"
  vpc_id           = module.vpc.vpc_id
  subnet_ids       = module.vpc.private_subnet_ids
  webapp_public_ip = module.clickhouse_ec2_instance.webapp_public_ip
}


 

--- ./requirements.txt ---

boto3==1.34.144
botocore==1.34.144
certifi==2024.7.4
clickhouse-connect==0.7.16
jmespath==1.0.1
lz4==4.3.3
python-dateutil==2.9.0.post0
pytz==2024.1
s3transfer==0.10.2
six==1.16.0
urllib3==2.2.2
zstandard==0.23.0


--- ./combined_project_files.txt ---



--- ./README.md ---

# deploy

## async inserts
- recommended best practice for ClickHouse
- https://clickhouse.com/docs/en/cloud/bestpractices/asynchronous-inserts
- multiple options available, choosing to not handle as a setting specific to each individual INSERT via Lambda function invocation, but rather as a Database user setting
  - not -- `INSERT INTO YourTable SETTINGS async_insert=1, wait_for_async_insert=1 VALUES (...)`
  - instead -- `ALTER USER default SETTINGS async_insert = 1`
- re ^, for our CLI prompts, can choose to either ask a user to provide a username and password for the ClickHouse DB, or just only allow a "default" user to be set up
  - either way, once we create a ClickHouse DB instance on the EC2 for the user, can run the `ALTER USER default SETTINGS async_insert = 1` command while in the `./clickhouse-client` on this EC2

## lambda function notes
option 1 - dynamoDB
get streamID from event - within the lambda_handler
connect to dynamoDB to get tableID for this streamID
tableID = table.__uuid__
{ streamID: tableID }
send a query to ch client to get tableName based on table.__uuid__
client.insert(tableName, data)

re testing this lambda function before abstracting code to terraform:
- setup env variable in the lambda configuration to hardcore a specific EC2 instance public url
- do the same with a dynamoDB table
  - add an item that has a real kinesis_stream_id and real table __uuid__ to this table

option 2 - clickhouse table

questions:
- how to handle lambda to/from dynamo errors?
- similarly, with our try/catch in the lambda... what actually happens if theres an error?
- how to handle timestamps and converting those to comparable format 
event_timestamp = datetime.fromisoformat(record['event_timestamp']).strftime('%Y-%m-%d %H:%M:%S')

--- ./combine_files.py ---

import os
import glob

def combine_files(root_dir, output_file):
    with open(output_file, 'w', encoding='utf-8') as outfile:
        for root, dirs, files in os.walk(root_dir):
            for file in files:
                if file.endswith(('.md', '.py', '.tf', '.txt')) and file not in ['terraform.tfstate', 'terraform.tfstate.backup']:
                    file_path = os.path.join(root, file)
                    outfile.write(f"\n\n--- {file_path} ---\n\n")
                    try:
                        with open(file_path, 'r', encoding='utf-8') as infile:
                            outfile.write(infile.read())
                    except UnicodeDecodeError:
                        outfile.write(f"Unable to read {file_path} - it might be a binary file.")

# Specify the root directory of your project
root_directory = '.'  # Current directory, change if needed

# Specify the output file name
output_file = 'combined_project_files.txt'

combine_files(root_directory, output_file)
print(f"All files have been combined into {output_file}")

--- ./lambda_function.py ---

import base64
import json

import logging
import clickhouse_connect
import os
import boto3
from boto3.dynamodb.conditions import Key

# Set up logging
logger = logging.getLogger()
logger.setLevel(logging.DEBUG)

# ClickHouse connection details
CLICKHOUSE_HOST = os.environ['CLICKHOUSE_HOST']
CLICKHOUSE_PORT = int(os.environ.get('CLICKHOUSE_PORT', 8123))
client = clickhouse_connect.get_client(host=CLICKHOUSE_HOST, port=CLICKHOUSE_PORT)

def create_dynamodb_client():
    return boto3.resource(
        'dynamodb',
        region_name='us-west-1'
    )

def get_table_id(dynamo_client, name, stream_id):
    table = dynamo_client.Table(name)
    response = table.query(
        KeyConditionExpression=Key('stream_id').eq(stream_id)
    )
    return response["Items"][0]["table_id"]

def get_table_name(table_id):
    res = client.query(f"""
        SELECT name
        FROM system.tables
        WHERE database = 'default'
        AND toString(uuid) = '{table_id}'
        """)
    return res.first_row[0]
            

def lambda_handler(event, context):
    logger.info(f"Lambda function invoked with event: {json.dumps(event)}")
    try:
        records_processed = 0
        for record in event['Records']:
            logger.info(f"Processing record: {record['kinesis']['sequenceNumber']}")
            logger.info(f"DECODED DATA: {record['kinesis']['data']}")
            
            # Kinesis data is base64 encoded
            payload = base64.b64decode(record['kinesis']['data'])
            logger.info(f"Decoded payload: {payload}")
            
            data = json.loads(payload)
            logger.info(f"Parsed data: {json.dumps(data)}")

            dynamodb_client = create_dynamodb_client()
            stream_id = record['eventSourceARN']
            table_id = get_table_id(dynamodb_client, 'stream_table_map', stream_id)
            table_name = get_table_name(table_id)
            
            # Process the data
            insert_data_to_clickhouse(data, table_name) # might be [data]
            records_processed += 1
        
        logger.info(f"Successfully processed {records_processed} records")
        return {
            'statusCode': 200,
            'body': json.dumps({'status': 'success', 'records_processed': records_processed})
        }
    except Exception as e:
        logger.error(f"Error processing records: {str(e)}", exc_info=True)
        return {
            'statusCode': 500,
            'body': json.dumps({'status': 'error', 'message': str(e)})
        }

def insert_data_to_clickhouse(data, tableName):
    logger.info(f"Data to client.insert: {data}")
    
    try:
        column_names = list(data.keys())
        rows = []
        for keys, vals in data.items():
            rows.append(vals)
    
    
        res = client.insert(tableName, [rows], column_names=column_names)
        #res = client.insert(tableName, [[rows[0]]], column_names=[column_names[0]])
        logger.info(f"inserted response: {res}")
    except Exception as e:
        logger.error(f"Error during ClickHouse query execution: {str(e)}", exc_info=True)
        raise



--- ./modules/dynamodb_instance/main.tf ---

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

resource "aws_dynamodb_table" "dynamodb_stream_table" {
  name           = "stream_table_map"
  billing_mode   = "PROVISIONED"
  read_capacity  = 3
  write_capacity = 3
  hash_key       = "stream_id"
  range_key      = "table_id"

  attribute {
    name = "stream_id"
    type = "S"
  }

  attribute {
    name = "table_id"
    type = "S"
  }

  global_secondary_index {
    name               = "TableIdIndex"
    hash_key           = "table_id"
    projection_type    = "ALL"
    read_capacity      = 1
    write_capacity     = 1
  }

  tags = {
    Name        = "dynamodb_stream_table"
  }
}

--- ./modules/clickhouse_ec2_instance/main.tf ---

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

data "aws_ami" "ubuntu" {
  most_recent = true
  owners      = ["099720109477"]

  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*"]
  }

  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }
}

resource "tls_private_key" "ssh_key" {
  algorithm = "RSA"
  rsa_bits  = 4096
}

resource "aws_key_pair" "generated_key" {
  key_name   = "capstone-key"
  public_key = tls_private_key.ssh_key.public_key_openssh
}

resource "local_file" "private_key" {
  content         = tls_private_key.ssh_key.private_key_pem
  filename        = "${path.module}/capstone-key.pem"
  file_permission = "0400"
}

resource "aws_security_group" "clickhouse_sg" {
  name        = "clickhouse-security-group"
  description = "Security group for Clickhouse EC2 instance"
  vpc_id      = var.vpc_id

  ingress {
    from_port   = 8123
    to_port     = 8123
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 8124
    to_port     = 8124
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "clickhouse-security-group"
  }
}

resource "aws_instance" "clickhouse_server" {
  ami                    = data.aws_ami.ubuntu.id
  instance_type          = "t2.micro"
  vpc_security_group_ids = [aws_security_group.clickhouse_sg.id]
  subnet_id              = var.subnet_id
  key_name               = aws_key_pair.generated_key.key_name

  tags = {
    Name = "ClickHouse-Server"
  }

  user_data = <<-EOF
            #!/bin/bash
            apt-get update
            apt-get install -y docker.io
            systemctl start docker
            systemctl enable docker
            sudo usermod -aG docker ubuntu
            docker pull jamesdrabinsky/helios-clickhouse-amd:dev
            docker run -d --name my-clickhouse-container --ulimit nofile=262144:262144 -p 8123:8123 -p 8443:8443 -p 9000:9000 -p 9440:9440 jamesdrabinsky/helios-clickhouse-amd:dev
            EOF
}

output "private_key" {
  value     = tls_private_key.ssh_key.private_key_pem
  sensitive = true
}

# output "webapp_public_ip" {
#   value = aws_instance.web_app.public_ip
# }

output "webapp_public_ip" {
  value = aws_instance.clickhouse_server.public_ip
}


# docker exec -it clickhouse-server clickhouse-client -- to run the clickhouse client

# WARNING: The requested image's platform (linux/arm64) does not match the detected host platform 
# (linux/amd64/v3) and no specific platform was requested

# docker pull clickhouse/clickhouse-server
# docker run -d --name my-clickhouse-container --ulimit nofile=262144:262144 -p 8124:8123 -p 9001:9000 clickhouse/clickhouse-server


--- ./modules/clickhouse_ec2_instance/variables.tf ---

variable "vpc_id" {
  description = "The ID of the VPC"
  type        = string
}

variable "subnet_id" {
  description = "The ID of the subnet where the instance will be launched"
  type        = string
}

--- ./modules/lambda/main.tf ---

provider "aws" {
  region  = "us-west-1"
  profile = "capstone-team4"
}

resource "aws_lambda_function" "kinesis_to_clickhouse" {
  filename      = "lambda_function.zip"
  function_name = "kinesis-to-clickhouse"
  role          = aws_iam_role.lambda_role.arn
  description   = "handler function for sending kinesis data to clickhouse"
  handler       = "lambda_function.lambda_handler"
  runtime       = "python3.12"
  # publish       = true

  source_code_hash = filebase64sha256("lambda_function.zip")

  environment {
    variables = {
      CLICKHOUSE_HOST = var.webapp_public_ip
      CLICKHOUSE_PORT = "8123"
    }
  }

  layers = [
    aws_lambda_layer_version.layer_content.arn
  ]

  vpc_config {
    subnet_ids         = var.subnet_ids
    security_group_ids = [aws_security_group.lambda_sg.id]
  }
}

resource "aws_security_group" "lambda_sg" {
  name        = "lambda-security-group"
  description = "Security group for Lambda function"
  vpc_id      = var.vpc_id

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

resource "aws_lambda_event_source_mapping" "kinesis_trigger" {
  event_source_arn  = "arn:aws:kinesis:us-west-1:767397811841:stream/MyKinesisDataStream"
  function_name     = aws_lambda_function.kinesis_to_clickhouse.arn
  starting_position = "LATEST"
  batch_size        = 3
  enabled           = true
}

resource "aws_iam_policy" "lambda_layer_policy" {
  name        = "lambda_layer_policy"
  path        = "/"
  description = "IAM policy for Lambda layer creation"

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "lambda:PublishLayerVersion",
          "lambda:GetLayerVersion",
          "lambda:DeleteLayerVersion"
        ]
        Resource = "arn:aws:lambda:us-west-1:659377685669:layer:*"
      }
    ]
  })
}

resource "aws_iam_role" "lambda_role" {
  name = "lambda_execution_role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "lambda.amazonaws.com"
        }
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "lambda_layer_policy" {
  role       = aws_iam_role.lambda_role.name
  policy_arn = aws_iam_policy.lambda_layer_policy.arn
}

resource "aws_iam_role_policy_attachment" "lambda_policy" {
  policy_arn = "arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
  role       = aws_iam_role.lambda_role.name
}

resource "aws_iam_role_policy_attachment" "kinesis_policy" {
  policy_arn = "arn:aws:iam::aws:policy/service-role/AWSLambdaKinesisExecutionRole"
  role       = aws_iam_role.lambda_role.name
}

resource "aws_iam_role_policy_attachment" "dynamodb_policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess"
  role       = aws_iam_role.lambda_role.name
}

resource "aws_lambda_layer_version" "layer_content" {
  filename   = "layer_content.zip"
  layer_name = "layer_content"

  compatible_runtimes = ["python3.12"]
}

# resource "aws_lambda_layer_version" "dotenv" {
#   filename   = "dotenv-5d44c573-0872-4ac3-bdc7-c9be6be9eea9.zip"
#   layer_name = "dotenv"

#   compatible_runtimes = ["python3.12"]
# }




# pip install python-dotenv -t ./dotenv/python/lib/python3.12/site-packages/
# pip install clickhouse-connect -t ./clickhouse-connect/python/lib/python3.12/site-packages/
# pip install boto3 -t ./boto3/python/lib/python3.12/site-packages/

# zip -r dotenv_layer.zip ./dotenv
# zip -r clickhouse_connect_layer.zip ./clickhouse-connect
# zip -r boto3_layer.zip ./boto3

# zip -r lambda_function.zip lambda_function.py


--- ./modules/lambda/variables.tf ---

variable "webapp_public_ip" {
  type = string
}


--- ./modules/vpc/main.tf ---

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

# vpc.tf

resource "aws_vpc" "main" {
  cidr_block           = "10.0.0.0/16"
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = "main-vpc"
  }
}

resource "aws_internet_gateway" "main" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = "main-igw"
  }
}

resource "aws_subnet" "public" {
  count             = 2
  vpc_id            = aws_vpc.main.id
  cidr_block        = "10.0.${count.index}.0/24"
  availability_zone = data.aws_availability_zones.available.names[count.index]

  tags = {
    Name = "Public Subnet ${count.index + 1}"
  }
}

resource "aws_subnet" "private" {
  count             = 2
  vpc_id            = aws_vpc.main.id
  cidr_block        = "10.0.${count.index + 10}.0/24"
  availability_zone = data.aws_availability_zones.available.names[count.index]

  tags = {
    Name = "Private Subnet ${count.index + 1}"
  }
}

resource "aws_eip" "nat" {
  count = 1
  vpc   = true

  tags = {
    Name = "NAT Gateway EIP"
  }
}

resource "aws_nat_gateway" "main" {
  count         = 1
  allocation_id = aws_eip.nat[0].id
  subnet_id     = aws_subnet.public[0].id

  tags = {
    Name = "Main NAT Gateway"
  }
}

resource "aws_route_table" "public" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name = "Public Route Table"
  }
}

resource "aws_route_table" "private" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block     = "0.0.0.0/0"
    nat_gateway_id = aws_nat_gateway.main[0].id
  }

  tags = {
    Name = "Private Route Table"
  }
}

resource "aws_route_table_association" "public" {
  count          = 2
  subnet_id      = aws_subnet.public[count.index].id
  route_table_id = aws_route_table.public.id
}

resource "aws_route_table_association" "private" {
  count          = 2
  subnet_id      = aws_subnet.private[count.index].id
  route_table_id = aws_route_table.private.id
}

data "aws_availability_zones" "available" {}

--- ./modules/flask_ec2_instance/main.tf ---

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

data "aws_ami" "ubuntu" {
  most_recent = true
  owners      = ["099720109477"]

  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*"]
  }

  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }
}

resource "tls_private_key" "ssh_key" {
  algorithm = "RSA"
  rsa_bits  = 4096
}

resource "aws_key_pair" "generated_key" {
  key_name   = "capstone-key1"
  public_key = tls_private_key.ssh_key.public_key_openssh
}

resource "local_file" "private_key" {
  content         = tls_private_key.ssh_key.private_key_pem
  filename        = "${path.module}/capstone-key.pem"
  file_permission = "0400"
}

resource "aws_security_group" "flask_sg" {
  name        = "flask-security-group"
  description = "Security group for flask EC2 instance"
  vpc_id      = var.vpc_id

  ingress {
    from_port   = 5000
    to_port     = 5000
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "flask-security-group"
  }
}

resource "aws_instance" "flask_server" {
  ami                    = data.aws_ami.ubuntu.id
  instance_type          = "t2.micro"
  vpc_security_group_ids = [aws_security_group.flask_sg.id]
  subnet_id              = var.subnet_id
  key_name               = aws_key_pair.generated_key.key_name
  tags = {
    Name = "Flask-Server"
  }

  user_data = <<-EOF
#!/bin/bash
exec > >(tee /var/log/user-data.log|logger -t user-data -s 2>/dev/console) 2>&1
echo "Starting user data script execution"

# Log the webapp_public_ip
echo "webapp_public_ip is: ${var.webapp_public_ip}"

apt-get update
apt-get install -y docker.io
echo "Docker installed"
systemctl start docker
systemctl enable docker

# Wait for Docker to be fully operational
echo "Waiting for Docker service to be fully operational..."
timeout=300  # 5 minutes timeout
end=$((SECONDS+timeout))

while [ $SECONDS -lt $end ]; do
    if docker info >/dev/null 2>&1; then
        echo "Docker is up and running"
        break
    else
        echo "Waiting for Docker to start... ($(($end-SECONDS)) seconds left)"
        sleep 5
    fi
done

if ! docker info >/dev/null 2>&1; then
    echo "Docker failed to start within the allotted time" >&2
    exit 1
fi

sudo usermod -aG docker ubuntu
echo "Added ubuntu user to docker group"

docker pull jamesdrabinsky/flask-frontend-app:latest
echo "Docker image pulled"

# Create a directory for the Dockerfile
mkdir -p /app
cd /app

# Create the Dockerfile
cat <<EOT > Dockerfile
FROM jamesdrabinsky/flask-frontend-app:latest
ENV CH_HOST=${var.webapp_public_ip}
EOT
echo "Dockerfile created"

# Build the new image
docker build -t flask-app-with-env .
echo "New Docker image built"

# Run the new image
docker run -d -p 5000:5000 --name flask-app flask-app-with-env
echo "Flask app container started"

echo "User data script execution completed"
EOF

  provisioner "local-exec" {
    command = <<EOT
      while ! nc -zv ${var.webapp_public_ip} 8123; do
        echo "Waiting for ClickHouse to be available..."
        sleep 2
      done
      echo "ClickHouse is up and running!"
    EOT
  }
}

output "private_key" {
  value     = tls_private_key.ssh_key.private_key_pem
  sensitive = true
}

# docker run -d -p 5000:5000 --name flask-app jamesdrabinsky/flask-frontend-app:latest

# docker exec -it flask-app
# echo "CH_HOST=${var.webapp_public_ip}" >> .env

# sudo cat /var/log/user-data.log
# sudo docker logs flask-app


--- ./modules/flask_ec2_instance/variables.tf ---

variable "webapp_public_ip" {
  type        = string
  description = "The public IP of the ClickHouse instance"
}
